# ğŸ“” DIÃRIO DOCLING - VERSÃƒO 2.0 OPERACIONAL

**VersÃ£o:** 2.0 (Otimizada para Uso DiÃ¡rio)  
**Data:** 26 de outubro de 2025  
**Status:** Pronto para ReconstruÃ§Ã£o PÃ³s-FormataÃ§Ã£o  
**Ambiente:** Windows 11 + WSL2 Ubuntu 22.04 + Docker + Dify + Ollama

---

## ğŸ¯ RESUMO EM 30 SEGUNDOS

Sistema de IA especialista local para Docling. Setup:
- **3 modelos LLM** (Granite, Qwen 4K, Qwen 32K)
- **1 modelo Embedding** (Nomic)
- **Interface RAG** via Dify
- **GPU RTX 3060** para inferÃªncia local
- **Zero APIs pagas**

---

## ğŸ“Š RÃPIDA REFERÃŠNCIA - USAR ESTE MODELO

| Caso de Uso | Modelo | Tempo |
|------------|--------|-------|
| Resposta rÃ¡pida | Granite 4.0 Micro | 1-3s âš¡ |
| CÃ³digo/DocumentaÃ§Ã£o | Qwen 4K | 2-5s âš™ï¸ |
| Documentos longos | Qwen 32K | 3-7s ğŸ“š |
| RAG/Busca | Nomic Embed | 10-50ms ğŸ” |

---

## ğŸ”§ QUICK SETUP - 4 FASES

### FASE 1: Ambiente Base (30 min)
```bash
# WSL2
wsl --list --verbose
wsl --import Ubuntu D:\wsl_distros\Ubuntu [backup-file]
# Editar: /etc/wsl.conf â†’ [user] default=diablo
wsl --shutdown

# Docker
sudo service docker start
sudo usermod -aG docker diablo
wsl --shutdown
```

### FASE 2: Dify + Docker (20 min)
```bash
cd /mnt/d/projetos/dify
git clone https://github.com/langgenius/dify.git
cd dify/docker
docker compose up -d
# Acesso: http://localhost
```

### FASE 3: Ollama + Modelos (30 min)
```bash
# Terminal 1 (DEDICADO):
ollama serve

# Terminal 2:
ollama pull granite4:micro        # 2.1GB
ollama pull qwen2.5-coder:14b     # 9GB
ollama pull qwen2.5-coder:14b-32k # 9GB (compartilha layers)
ollama pull nomic-embed-text      # 274MB
```

### FASE 4: Dify + Modelos (10 min)
```
Dify Web â†’ Settings â†’ Model Provider â†’ Ollama
Base URL: http://[IP_WSL]:11434
Get Models: Deve listar 4 modelos âœ…
```

---

## ğŸ§  ESPECIFICAÃ‡Ã•ES DOS MODELOS

### 1. IBM Granite 4.0 Micro âš¡
- **Tamanho:** 3.4B params, 2.1GB disco
- **VRAM:** 2-3GB
- **Velocidade:** 80-100 tok/s
- **Uso:** Respostas rÃ¡pidas, perguntas bÃ¡sicas
- **LatÃªncia:** 1-3s

### 2. Qwen2.5-Coder 14B (4K) âš™ï¸
- **Tamanho:** 14B params, 9GB disco
- **VRAM:** 4.2GB
- **Velocidade:** 30-50 tok/s
- **Uso:** CÃ³digo, docs tÃ©cnicas, 4K contexto
- **LatÃªncia:** 2-5s

### 3. Qwen2.5-Coder 14B-32K (32K) ğŸ“š
- **Tamanho:** 14B params, 9GB disco (compartilha)
- **VRAM:** 5-5.5GB
- **Velocidade:** 25-40 tok/s
- **Uso:** Documentos longos, 32K contexto
- **LatÃªncia:** 3-7s

### 4. Nomic Embed Text ğŸ”
- **Tamanho:** 137M params, 274MB disco
- **VRAM:** 300-500MB
- **Velocidade:** 500-1000 embeddings/s
- **Uso:** RAG, busca semÃ¢ntica, indexaÃ§Ã£o
- **LatÃªncia:** 10-50ms

---

## ğŸ›  HARDWARE DISPONÃVEL

```
ğŸ’» CPU: Intel i7-11800H (11th Gen)
ğŸ® GPU: NVIDIA RTX 3060 Laptop (6GB VRAM)
ğŸ§  RAM: 40GB DDR4
ğŸ’¾ Disco: D:\ (workspace)
```

**RestriÃ§Ã£o CrÃ­tica:** 6GB VRAM GPU
- Granite + Nomic = 2.5GB + 0.5MB = âœ… Fits
- Granite + Qwen4K = 2.5GB + 4.2GB = âŒ Tight (13% spillover)
- Qwen4K + Nomic = 4.2GB + 0.5GB = âœ… OK
- Qwen32K = 5.5GB + 0.5GB swap = âš ï¸ Use sparingly

---

## ğŸ“‹ HEURÃSTICAS CRÃTICAS

| # | Regra | Quando |
|---|-------|--------|
| 1 | `wsl --list --verbose` | Antes de qualquer comando wsl |
| 2 | Editar `/etc/wsl.conf` | ApÃ³s `wsl --import` |
| 3 | `wsl --shutdown` | ApÃ³s config/grupo/perms |
| 4 | `sudo service docker start` | PRIMEIRA coisa no WSL |
| 5 | Adicionar grupo â†’ shutdown | Depois de `usermod -aG` |
| 6 | `ollama serve` em terminal dedicado | Sempre rodando |
| 7 | Nomes de `ollama.com/library` | Ao puxar modelos |

---

## ğŸš€ FLUXO DE DECISÃƒO - QUAL MODELO USAR

```
Precisa resposta agora?
â”œâ”€ SIM â†’ Granite (2-3s)
â””â”€ NÃƒO â†“

Ã‰ sobre cÃ³digo/docs tÃ©cnicas?
â”œâ”€ NÃƒO â†’ Granite
â””â”€ SIM â†“

Doc Ã© < 3000 palavras?
â”œâ”€ SIM â†’ Qwen 4K (2-5s)
â””â”€ NÃƒO â†’ Qwen 32K (3-7s)

+ Sempre usar Nomic para busca RAG
```

---

## ğŸ“ ESTRUTURA DE PASTAS

```
D:\
â”œâ”€â”€ wsl_distros\Ubuntu\          â† WSL2
â”œâ”€â”€ Projetos\
â”‚   â”œâ”€â”€ dify\                    â† Docker Compose
â”‚   â””â”€â”€ docling\                 â† Repo Docling
â”œâ”€â”€ Documents\
â”‚   â””â”€â”€ Docling_RAG_Project\     â† Obsidian Vault
â””â”€â”€ diario_de_bordo\             â† Este arquivo
```

---

## ğŸ”— LINKS CRÃTICOS

- **Docling:** https://github.com/docling-project/docling
- **Ollama:** https://ollama.com/library
- **Dify:** https://docs.dify.ai/
- **Granite:** https://www.ibm.com/granite/
- **Qwen:** https://github.com/QwenLM/Qwen

---

## ğŸ“ ENDEREÃ‡OS OPERACIONAIS

- **Dify Web:** `http://localhost`
- **Ollama API:** `http://localhost:11434`
- **WSL2 Ubuntu:** Usuario `diablo`
- **Modelos Ollama:**
  - `granite4:micro`
  - `qwen2.5-coder:14b`
  - `qwen2.5-coder:14b-32k`
  - `nomic-embed-text`

---

## â° TEMPOS

| Tarefa | Tempo |
|--------|-------|
| Setup Completo (primeira vez) | 4-6h |
| Setup DiÃ¡rio (apÃ³s shutdown) | 5 min |
| Troubleshoot tÃ­pico | 10-15 min |
| Resposta Granite | 1-3s |
| Resposta Qwen 4K | 2-5s |
| Resposta Qwen 32K | 3-7s |

---

## âœ¨ RESULTADO

âœ… IA especialista local 100%  
âœ… 3 LLMs + 1 Embedding  
âœ… Interface web (Dify)  
âœ… Zero APIs pagas  
âœ… Privacidade total  
âœ… GPU acelerado  

---

## ğŸ”„ MANUTENÃ‡ÃƒO DIÃRIA

1. Abrir WSL terminal
2. `sudo service docker start`
3. Abrir novo terminal: `ollama serve`
4. Abrir novo terminal: trabalhar normalmente
5. Se fechar: repetir desde passo 1

---

## ğŸ“ REGISTRO DE MUDANÃ‡AS

**v2.0 (26/10/2025):**
- Removido: 60% de conteÃºdo redundante
- Adicionado: Fluxo de decisÃ£o modelos
- Otimizado: Tabelas de referÃªncia rÃ¡pida
- Mantido: Todos os 7 erros + heurÃ­sticas

---

**STATUS:** âœ… Pronto para ProduÃ§Ã£o  
**ÃšLTIMA ATUALIZAÃ‡ÃƒO:** 26/10/2025 01:15  
**MANTIDO POR:** Auto (Obsidian + DiÃ¡rio)

